{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 14:23:11.529052: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-14 14:23:11.529711: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-14 14:23:11.532024: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-14 14:23:11.539101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-14 14:23:11.554168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-14 14:23:11.559575: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-14 14:23:11.570293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-14 14:23:12.186730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the CartPole Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actor and critic networks\n",
    "actor = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(env.reset()[0].shape),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "])\n",
    "\n",
    "critic = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss functions\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcc/miniconda3/envs/rl-env/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at 11\n",
      "Episode 0, Reward: 11.0\n",
      "Done at 12\n",
      "Done at 14\n",
      "Done at 19\n",
      "Done at 16\n",
      "Done at 11\n",
      "Done at 31\n",
      "Done at 24\n",
      "Done at 12\n",
      "Done at 18\n",
      "Done at 42\n",
      "Episode 10, Reward: 42.0\n",
      "Done at 24\n",
      "Done at 24\n",
      "Done at 15\n",
      "Done at 11\n",
      "Done at 19\n",
      "Done at 14\n",
      "Done at 11\n",
      "Done at 22\n",
      "Done at 12\n",
      "Done at 15\n",
      "Episode 20, Reward: 15.0\n",
      "Done at 10\n",
      "Done at 39\n",
      "Done at 9\n",
      "Done at 22\n",
      "Done at 31\n",
      "Done at 35\n",
      "Done at 36\n",
      "Done at 13\n",
      "Done at 35\n",
      "Done at 31\n",
      "Episode 30, Reward: 31.0\n",
      "Done at 14\n",
      "Done at 23\n",
      "Done at 13\n",
      "Done at 13\n",
      "Done at 20\n",
      "Done at 14\n",
      "Done at 20\n",
      "Done at 16\n",
      "Done at 18\n",
      "Done at 26\n",
      "Episode 40, Reward: 26.0\n",
      "Done at 10\n",
      "Done at 15\n",
      "Done at 36\n",
      "Done at 11\n",
      "Done at 20\n",
      "Done at 16\n",
      "Done at 60\n",
      "Done at 36\n",
      "Done at 67\n",
      "Done at 19\n",
      "Episode 50, Reward: 19.0\n",
      "Done at 24\n",
      "Done at 65\n",
      "Done at 43\n",
      "Done at 29\n",
      "Done at 33\n",
      "Done at 56\n",
      "Done at 45\n",
      "Done at 63\n",
      "Done at 34\n",
      "Done at 30\n",
      "Episode 60, Reward: 30.0\n",
      "Done at 33\n",
      "Done at 22\n",
      "Done at 71\n",
      "Done at 46\n",
      "Done at 84\n",
      "Done at 32\n",
      "Done at 18\n",
      "Done at 35\n",
      "Done at 31\n",
      "Done at 21\n",
      "Episode 70, Reward: 21.0\n",
      "Done at 20\n",
      "Done at 32\n",
      "Done at 70\n",
      "Done at 156\n",
      "Done at 22\n",
      "Done at 49\n",
      "Done at 30\n",
      "Done at 36\n",
      "Done at 62\n",
      "Done at 35\n",
      "Episode 80, Reward: 35.0\n",
      "Done at 25\n",
      "Done at 36\n",
      "Done at 89\n",
      "Done at 44\n",
      "Done at 39\n",
      "Done at 28\n",
      "Done at 59\n",
      "Done at 29\n",
      "Done at 40\n",
      "Done at 41\n",
      "Episode 90, Reward: 41.0\n",
      "Done at 23\n",
      "Done at 54\n",
      "Done at 44\n",
      "Done at 51\n",
      "Done at 43\n",
      "Done at 50\n",
      "Done at 46\n",
      "Done at 38\n",
      "Done at 86\n",
      "Done at 62\n",
      "Episode 100, Reward: 62.0\n",
      "Done at 83\n",
      "Done at 54\n",
      "Done at 163\n",
      "Done at 63\n",
      "Done at 26\n",
      "Done at 82\n",
      "Done at 67\n",
      "Done at 115\n",
      "Done at 108\n",
      "Done at 47\n",
      "Episode 110, Reward: 47.0\n",
      "Done at 41\n",
      "Done at 82\n",
      "Done at 51\n",
      "Done at 36\n",
      "Done at 178\n",
      "Done at 41\n",
      "Done at 38\n",
      "Done at 57\n",
      "Done at 30\n",
      "Done at 41\n",
      "Episode 120, Reward: 41.0\n",
      "Done at 36\n",
      "Done at 30\n",
      "Done at 39\n",
      "Done at 73\n",
      "Done at 16\n",
      "Done at 25\n",
      "Done at 18\n",
      "Done at 44\n",
      "Done at 29\n",
      "Done at 25\n",
      "Episode 130, Reward: 25.0\n",
      "Done at 51\n",
      "Done at 43\n",
      "Done at 33\n",
      "Done at 51\n",
      "Done at 44\n",
      "Done at 39\n",
      "Done at 36\n",
      "Done at 39\n",
      "Done at 57\n",
      "Done at 28\n",
      "Episode 140, Reward: 28.0\n",
      "Done at 41\n",
      "Done at 53\n",
      "Done at 84\n",
      "Done at 37\n",
      "Done at 47\n",
      "Done at 33\n",
      "Done at 49\n",
      "Done at 102\n",
      "Done at 34\n",
      "Done at 63\n",
      "Episode 150, Reward: 63.0\n",
      "Done at 35\n",
      "Done at 23\n",
      "Done at 60\n",
      "Done at 34\n",
      "Done at 29\n",
      "Done at 36\n",
      "Done at 30\n",
      "Done at 61\n",
      "Done at 32\n",
      "Done at 59\n",
      "Episode 160, Reward: 59.0\n",
      "Done at 32\n",
      "Done at 36\n",
      "Done at 43\n",
      "Done at 83\n",
      "Done at 34\n",
      "Done at 58\n",
      "Done at 27\n",
      "Done at 72\n",
      "Done at 50\n",
      "Done at 102\n",
      "Episode 170, Reward: 102.0\n",
      "Done at 60\n",
      "Done at 81\n",
      "Done at 164\n",
      "Done at 71\n",
      "Done at 139\n",
      "Done at 76\n",
      "Done at 105\n",
      "Done at 78\n",
      "Done at 130\n",
      "Done at 80\n",
      "Episode 180, Reward: 80.0\n",
      "Done at 91\n",
      "Done at 145\n",
      "Done at 96\n",
      "Done at 116\n",
      "Done at 59\n",
      "Done at 136\n",
      "Done at 145\n",
      "Done at 97\n",
      "Done at 106\n",
      "Done at 178\n",
      "Episode 190, Reward: 178.0\n",
      "Done at 299\n",
      "Done at 153\n",
      "Done at 107\n",
      "Done at 163\n",
      "Done at 101\n",
      "Done at 150\n",
      "Done at 180\n",
      "Done at 337\n",
      "Done at 304\n",
      "Done at 120\n",
      "Episode 200, Reward: 120.0\n",
      "Done at 67\n",
      "Done at 107\n",
      "Done at 201\n",
      "Done at 307\n",
      "Done at 283\n",
      "Done at 174\n",
      "Done at 135\n",
      "Done at 230\n",
      "Done at 441\n",
      "Done at 266\n",
      "Episode 210, Reward: 266.0\n",
      "Done at 212\n",
      "Done at 239\n",
      "Done at 244\n",
      "Done at 351\n",
      "Done at 372\n",
      "Done at 326\n",
      "Done at 760\n",
      "Done at 674\n",
      "Done at 258\n",
      "Done at 176\n",
      "Episode 220, Reward: 176.0\n",
      "Done at 245\n",
      "Done at 251\n",
      "Done at 212\n",
      "Done at 253\n",
      "Done at 351\n",
      "Done at 841\n",
      "Done at 471\n",
      "Done at 424\n",
      "Done at 289\n",
      "Done at 223\n",
      "Episode 230, Reward: 223.0\n",
      "Done at 232\n",
      "Done at 245\n",
      "Done at 225\n",
      "Done at 262\n",
      "Done at 287\n",
      "Done at 375\n",
      "Done at 491\n",
      "Done at 268\n",
      "Done at 79\n",
      "Done at 122\n",
      "Episode 240, Reward: 122.0\n",
      "Done at 373\n",
      "Done at 290\n",
      "Done at 71\n",
      "Done at 125\n",
      "Done at 110\n",
      "Done at 374\n",
      "Done at 123\n",
      "Done at 31\n",
      "Done at 28\n",
      "Done at 101\n",
      "Episode 250, Reward: 101.0\n",
      "Done at 156\n",
      "Done at 57\n",
      "Done at 65\n",
      "Done at 79\n",
      "Done at 15\n",
      "Done at 100\n",
      "Done at 30\n",
      "Done at 43\n",
      "Done at 74\n",
      "Done at 178\n",
      "Episode 260, Reward: 178.0\n",
      "Done at 76\n",
      "Done at 123\n",
      "Done at 30\n",
      "Done at 220\n",
      "Done at 177\n",
      "Done at 77\n",
      "Done at 109\n",
      "Done at 52\n",
      "Done at 16\n",
      "Done at 50\n",
      "Episode 270, Reward: 50.0\n",
      "Done at 63\n",
      "Done at 32\n",
      "Done at 37\n",
      "Done at 12\n",
      "Done at 28\n",
      "Done at 62\n",
      "Done at 47\n",
      "Done at 60\n",
      "Done at 10\n",
      "Done at 37\n",
      "Episode 280, Reward: 37.0\n",
      "Done at 41\n",
      "Done at 26\n",
      "Done at 24\n",
      "Done at 40\n",
      "Done at 69\n",
      "Done at 43\n",
      "Done at 28\n",
      "Done at 18\n",
      "Done at 25\n",
      "Done at 16\n",
      "Episode 290, Reward: 16.0\n",
      "Done at 122\n",
      "Done at 30\n",
      "Done at 10\n",
      "Done at 12\n",
      "Done at 12\n",
      "Done at 33\n",
      "Done at 38\n",
      "Done at 65\n",
      "Done at 40\n",
      "Done at 15\n",
      "Episode 300, Reward: 15.0\n",
      "Done at 56\n",
      "Done at 34\n",
      "Done at 30\n",
      "Done at 45\n",
      "Done at 41\n",
      "Done at 10\n",
      "Done at 41\n",
      "Done at 10\n",
      "Done at 51\n",
      "Done at 15\n",
      "Episode 310, Reward: 15.0\n",
      "Done at 40\n",
      "Done at 12\n",
      "Done at 9\n",
      "Done at 11\n",
      "Done at 12\n",
      "Done at 10\n",
      "Done at 17\n",
      "Done at 13\n",
      "Done at 9\n",
      "Done at 12\n",
      "Episode 320, Reward: 12.0\n",
      "Done at 12\n",
      "Done at 11\n",
      "Done at 10\n",
      "Done at 10\n",
      "Done at 19\n",
      "Done at 11\n",
      "Done at 32\n",
      "Done at 12\n",
      "Done at 37\n",
      "Done at 11\n",
      "Episode 330, Reward: 11.0\n",
      "Done at 14\n",
      "Done at 33\n",
      "Done at 16\n",
      "Done at 33\n",
      "Done at 13\n",
      "Done at 12\n",
      "Done at 34\n",
      "Done at 28\n",
      "Done at 11\n",
      "Done at 13\n",
      "Episode 340, Reward: 13.0\n",
      "Done at 11\n",
      "Done at 19\n",
      "Done at 11\n",
      "Done at 41\n",
      "Done at 13\n",
      "Done at 49\n",
      "Done at 16\n",
      "Done at 14\n",
      "Done at 28\n",
      "Done at 42\n",
      "Episode 350, Reward: 42.0\n",
      "Done at 12\n",
      "Done at 11\n",
      "Done at 11\n",
      "Done at 10\n",
      "Done at 36\n",
      "Done at 12\n",
      "Done at 12\n",
      "Done at 11\n",
      "Done at 37\n",
      "Done at 11\n",
      "Episode 360, Reward: 11.0\n",
      "Done at 23\n",
      "Done at 11\n",
      "Done at 26\n",
      "Done at 28\n",
      "Done at 14\n",
      "Done at 20\n",
      "Done at 11\n",
      "Done at 42\n",
      "Done at 28\n",
      "Done at 10\n",
      "Episode 370, Reward: 10.0\n",
      "Done at 24\n",
      "Done at 34\n",
      "Done at 11\n",
      "Done at 12\n",
      "Done at 12\n",
      "Done at 37\n",
      "Done at 10\n",
      "Done at 10\n",
      "Done at 12\n",
      "Done at 27\n",
      "Episode 380, Reward: 27.0\n",
      "Done at 11\n",
      "Done at 27\n",
      "Done at 15\n",
      "Done at 9\n",
      "Done at 10\n",
      "Done at 17\n",
      "Done at 17\n",
      "Done at 35\n",
      "Done at 13\n",
      "Done at 9\n",
      "Episode 390, Reward: 9.0\n",
      "Done at 11\n",
      "Done at 17\n",
      "Done at 12\n",
      "Done at 11\n",
      "Done at 11\n",
      "Done at 10\n",
      "Done at 10\n",
      "Done at 10\n",
      "Done at 10\n",
      "Done at 11\n",
      "Episode 400, Reward: 11.0\n",
      "Done at 12\n",
      "Done at 24\n",
      "Done at 13\n",
      "Done at 28\n",
      "Done at 9\n",
      "Done at 13\n",
      "Done at 10\n",
      "Done at 12\n",
      "Done at 10\n",
      "Done at 33\n",
      "Episode 410, Reward: 33.0\n",
      "Done at 9\n",
      "Done at 24\n",
      "Done at 17\n",
      "Done at 10\n",
      "Done at 41\n",
      "Done at 38\n",
      "Done at 16\n",
      "Done at 39\n",
      "Done at 12\n",
      "Done at 38\n",
      "Episode 420, Reward: 38.0\n",
      "Done at 12\n",
      "Done at 10\n",
      "Done at 29\n",
      "Done at 10\n",
      "Done at 38\n",
      "Done at 45\n",
      "Done at 102\n",
      "Done at 12\n",
      "Done at 9\n",
      "Done at 52\n",
      "Episode 430, Reward: 52.0\n",
      "Done at 58\n",
      "Done at 37\n",
      "Done at 13\n",
      "Done at 15\n",
      "Done at 42\n",
      "Done at 35\n",
      "Done at 14\n",
      "Done at 27\n",
      "Done at 18\n",
      "Done at 28\n",
      "Episode 440, Reward: 28.0\n",
      "Done at 13\n",
      "Done at 30\n",
      "Done at 62\n",
      "Done at 43\n",
      "Done at 38\n",
      "Done at 110\n",
      "Done at 62\n",
      "Done at 33\n",
      "Done at 14\n",
      "Done at 30\n",
      "Episode 450, Reward: 30.0\n",
      "Done at 37\n",
      "Done at 12\n",
      "Done at 14\n",
      "Done at 37\n",
      "Done at 23\n",
      "Done at 21\n",
      "Done at 43\n",
      "Done at 40\n",
      "Done at 14\n",
      "Done at 41\n",
      "Episode 460, Reward: 41.0\n",
      "Done at 33\n",
      "Done at 46\n",
      "Done at 46\n",
      "Done at 22\n",
      "Done at 41\n",
      "Done at 48\n",
      "Done at 26\n",
      "Done at 50\n",
      "Done at 32\n",
      "Done at 10\n",
      "Episode 470, Reward: 10.0\n",
      "Done at 24\n",
      "Done at 10\n",
      "Done at 64\n",
      "Done at 28\n",
      "Done at 14\n",
      "Done at 48\n",
      "Done at 11\n",
      "Done at 12\n",
      "Done at 21\n",
      "Done at 17\n",
      "Episode 480, Reward: 17.0\n",
      "Done at 23\n",
      "Done at 30\n",
      "Done at 59\n",
      "Done at 19\n",
      "Done at 34\n",
      "Done at 22\n",
      "Done at 9\n",
      "Done at 12\n",
      "Done at 30\n",
      "Done at 13\n",
      "Episode 490, Reward: 13.0\n",
      "Done at 19\n",
      "Done at 26\n",
      "Done at 62\n",
      "Done at 38\n",
      "Done at 225\n",
      "Done at 54\n",
      "Done at 73\n",
      "Done at 75\n",
      "Done at 104\n",
      "Done at 37\n",
      "Episode 500, Reward: 37.0\n",
      "Done at 19\n",
      "Done at 232\n",
      "Done at 47\n",
      "Done at 211\n",
      "Done at 17\n",
      "Done at 47\n",
      "Done at 87\n",
      "Done at 53\n",
      "Done at 69\n",
      "Done at 38\n",
      "Episode 510, Reward: 38.0\n",
      "Done at 37\n",
      "Done at 66\n",
      "Done at 61\n",
      "Done at 44\n",
      "Done at 33\n",
      "Done at 30\n",
      "Done at 13\n",
      "Done at 48\n",
      "Done at 40\n",
      "Done at 195\n",
      "Episode 520, Reward: 195.0\n",
      "Done at 19\n",
      "Done at 45\n",
      "Done at 31\n",
      "Done at 25\n",
      "Done at 122\n",
      "Done at 218\n",
      "Done at 48\n",
      "Done at 373\n",
      "Done at 463\n",
      "Done at 600\n",
      "Episode 530, Reward: 600.0\n",
      "Done at 333\n",
      "Done at 392\n",
      "Done at 421\n",
      "Done at 402\n",
      "Done at 388\n",
      "Done at 591\n",
      "Done at 586\n",
      "Done at 630\n",
      "Done at 805\n",
      "Done at 882\n",
      "Episode 540, Reward: 882.0\n",
      "Done at 451\n",
      "Done at 462\n",
      "Done at 911\n",
      "Done at 1708\n",
      "Done at 1505\n",
      "Done at 1410\n",
      "Done at 1288\n",
      "Done at 752\n",
      "Done at 722\n",
      "Done at 1692\n",
      "Episode 550, Reward: 1692.0\n",
      "Done at 1739\n",
      "Done at 567\n",
      "Done at 344\n",
      "Done at 716\n",
      "Done at 352\n",
      "Done at 416\n",
      "Done at 692\n",
      "Done at 694\n",
      "Done at 1493\n",
      "Done at 596\n",
      "Episode 560, Reward: 596.0\n",
      "Done at 887\n",
      "Done at 595\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    \"actor_loss\": [],\n",
    "    \"critic_loss\": [],\n",
    "    \"advantage\": [],\n",
    "    \"state\": [],\n",
    "    \"action\": [],\n",
    "    \"episode\": []\n",
    "}\n",
    "# Main training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        for t in range(1, 10000):  # Limit the number of time steps\n",
    "            # Choose an action using the actor\n",
    "            action_probs = actor(np.array([state]))\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
    "\n",
    "            # Take the chosen action and observe the next state and reward\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Compute the advantage\n",
    "            state_value = critic(np.array([state]))[0, 0]\n",
    "            next_state_value = critic(np.array([next_state]))[0, 0]\n",
    "            advantage = reward + gamma * next_state_value - state_value\n",
    "\n",
    "            # Compute actor and critic losses\n",
    "            actor_loss = -tf.math.log(action_probs[0, action]) * advantage\n",
    "            critic_loss = tf.square(advantage)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update actor and critic\n",
    "            actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "            critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "\n",
    "            history[\"action\"].append(action)\n",
    "            history[\"actor_loss\"].append(actor_loss.numpy())\n",
    "            history[\"critic_loss\"].append(critic_loss.numpy())\n",
    "            history[\"state\"].append(np.array([state]))\n",
    "            history[\"advantage\"].append(advantage.numpy())\n",
    "            history[\"episode\"].append(episode)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(f\"Done at {t}\")\n",
    "                break\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_loss</th>\n",
       "      <th>critic_loss</th>\n",
       "      <th>advantage</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775312</td>\n",
       "      <td>1.237291</td>\n",
       "      <td>1.112336</td>\n",
       "      <td>[[0.028243657, -0.008713742, -0.04648111, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.695130</td>\n",
       "      <td>1.005937</td>\n",
       "      <td>1.002964</td>\n",
       "      <td>[[0.028243657, -0.008713742, -0.04648111, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.772181</td>\n",
       "      <td>1.240795</td>\n",
       "      <td>1.113910</td>\n",
       "      <td>[[0.028243657, -0.008713742, -0.04648111, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.844709</td>\n",
       "      <td>1.490984</td>\n",
       "      <td>1.221058</td>\n",
       "      <td>[[0.028243657, -0.008713742, -0.04648111, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.913361</td>\n",
       "      <td>1.754193</td>\n",
       "      <td>1.324459</td>\n",
       "      <td>[[0.028243657, -0.008713742, -0.04648111, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22018</th>\n",
       "      <td>-0.236335</td>\n",
       "      <td>0.107310</td>\n",
       "      <td>-0.327581</td>\n",
       "      <td>[[0.0041085887, -0.035895947, -0.0006111813, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22019</th>\n",
       "      <td>-0.148713</td>\n",
       "      <td>0.049983</td>\n",
       "      <td>-0.223570</td>\n",
       "      <td>[[0.0041085887, -0.035895947, -0.0006111813, -...</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22020</th>\n",
       "      <td>-0.115961</td>\n",
       "      <td>0.025781</td>\n",
       "      <td>-0.160564</td>\n",
       "      <td>[[0.0041085887, -0.035895947, -0.0006111813, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22021</th>\n",
       "      <td>-0.037960</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>-0.052535</td>\n",
       "      <td>[[0.0041085887, -0.035895947, -0.0006111813, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22022</th>\n",
       "      <td>0.064632</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.089402</td>\n",
       "      <td>[[0.0041085887, -0.035895947, -0.0006111813, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22023 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actor_loss  critic_loss  advantage  \\\n",
       "0        0.775312     1.237291   1.112336   \n",
       "1        0.695130     1.005937   1.002964   \n",
       "2        0.772181     1.240795   1.113910   \n",
       "3        0.844709     1.490984   1.221058   \n",
       "4        0.913361     1.754193   1.324459   \n",
       "...           ...          ...        ...   \n",
       "22018   -0.236335     0.107310  -0.327581   \n",
       "22019   -0.148713     0.049983  -0.223570   \n",
       "22020   -0.115961     0.025781  -0.160564   \n",
       "22021   -0.037960     0.002760  -0.052535   \n",
       "22022    0.064632     0.007993   0.089402   \n",
       "\n",
       "                                                   state  action  episode  \n",
       "0      [[0.028243657, -0.008713742, -0.04648111, -0.0...       1        0  \n",
       "1      [[0.028243657, -0.008713742, -0.04648111, -0.0...       0        0  \n",
       "2      [[0.028243657, -0.008713742, -0.04648111, -0.0...       1        0  \n",
       "3      [[0.028243657, -0.008713742, -0.04648111, -0.0...       1        0  \n",
       "4      [[0.028243657, -0.008713742, -0.04648111, -0.0...       1        0  \n",
       "...                                                  ...     ...      ...  \n",
       "22018  [[0.0041085887, -0.035895947, -0.0006111813, -...       1      999  \n",
       "22019  [[0.0041085887, -0.035895947, -0.0006111813, -...       0      999  \n",
       "22020  [[0.0041085887, -0.035895947, -0.0006111813, -...       1      999  \n",
       "22021  [[0.0041085887, -0.035895947, -0.0006111813, -...       1      999  \n",
       "22022  [[0.0041085887, -0.035895947, -0.0006111813, -...       1      999  \n",
       "\n",
       "[22023 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "\n",
    "render_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "def render_episode(env: gym.Env, actor: tf.keras.Model, max_steps: int):\n",
    "  state, info = env.reset()\n",
    "  state = tf.constant(state, dtype=tf.float32)\n",
    "  screen = env.render()\n",
    "  images = [Image.fromarray(screen)]\n",
    "\n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs = actor(state)\n",
    "    action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
    "    # action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 10 steps\n",
    "    if i % 10 == 0:\n",
    "      screen = env.render()\n",
    "      images.append(Image.fromarray(screen))\n",
    "\n",
    "    if done:\n",
    "      print(f\"Done at {i}\")\n",
    "      break\n",
    "\n",
    "  return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(render_env, actor, 100000)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(image_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
